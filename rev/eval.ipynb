{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, brunnermunzel\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norationale = pd.read_json('learned-NQ-test-norationale-predicted.jsonl', lines=True)\n",
    "df_rationale = pd.read_json('learned-NQ-test-rationale-predicted.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 29896, 29906, 29941]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer(\"123\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_scores = []\n",
    "for i, row in df_rationale.iterrows():\n",
    "    y_true.append(row['golden_judge'])\n",
    "    score = row['rationale_log_prob'] - df_norationale.iloc[i]['norationale_log_prob']\n",
    "    seq_length = len(llama_tokenizer(row['golden_answer']).input_ids)\n",
    "    score = score / seq_length\n",
    "    y_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U test\n",
      "MannwhitneyuResult(statistic=4818571.5, pvalue=1.2513239616879862e-18)\n",
      "1.0309257502721478\n",
      "1.394922664116375\n"
     ]
    }
   ],
   "source": [
    "pos_scores = [score for score, label in zip(y_scores, y_true) if label == 1]\n",
    "neg_scores = [score for score, label in zip(y_scores, y_true) if label == 0]\n",
    "\n",
    "print(\"Mann-Whitney U test\")\n",
    "print(mannwhitneyu(pos_scores, neg_scores, alternative='less'))\n",
    "\n",
    "print(np.mean(pos_scores))\n",
    "print(np.mean(neg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc 0.7350993377483444\n",
      "Best threshold -4.749399185175001\n",
      "0.8474162277688726\n",
      "Best precision 0.7352317880794702\n",
      "Best recall 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1999\n",
      "           1       0.74      1.00      0.85      5551\n",
      "\n",
      "    accuracy                           0.74      7550\n",
      "   macro avg       0.37      0.50      0.42      7550\n",
      "weighted avg       0.54      0.74      0.62      7550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precs, recalls, ths = metrics.precision_recall_curve(y_true, y_scores)\n",
    "# rank by best F1\n",
    "fscores = []\n",
    "for prec, recall in zip(precs, recalls):\n",
    "    if prec + recall == 0:\n",
    "        fscores.append(0)\n",
    "    else:\n",
    "        fscores.append(2 * prec * recall / (prec + recall))\n",
    "accs = []\n",
    "for th in ths:\n",
    "    y_pred = [1 if score > th else 0 for score in y_scores]\n",
    "    accs.append(metrics.accuracy_score(y_true, y_pred))\n",
    "print(\"Best acc\", max(accs))\n",
    "\n",
    "\n",
    "fscores = np.array(fscores)\n",
    "best_th = ths[np.argmax(fscores)]\n",
    "print(\"Best threshold\", best_th)\n",
    "\n",
    "print(max(fscores))\n",
    "\n",
    "for prec, recall, th in zip(precs, recalls, ths):\n",
    "    if th == best_th:\n",
    "        print(\"Best precision\", prec)\n",
    "        print(\"Best recall\", recall)\n",
    "        break\n",
    "\n",
    "y_pred = [1 if score > best_th else 0 for score in y_scores]\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.73523179, 1.73501657, 1.73480134, ..., 0.        , 0.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precs + recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7350993377483444\n"
     ]
    }
   ],
   "source": [
    "print(max(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
